{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f90c0b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms as T\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "214cf033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "# check torch cuda is available\n",
    "print(\"Torch CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94270dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.MD  axial_t1wce_2_class\tcoronal_t1wce_2_class  sagittal_t1wce_2_class\n"
     ]
    }
   ],
   "source": [
    "!ls /mnt/d/object_detect_tracking/camera-viewer/data/brain_tumor_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80d535f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms as T\n",
    "\n",
    "class YoloDataset(Dataset):\n",
    "    def __init__(self, data_dir, img_size=320, transform=None, mode='train'):\n",
    "        self.data_dir = data_dir\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        self._prepare_dataset()\n",
    "\n",
    "    def _check_and_clean(self, img_dir, label_dir):\n",
    "        for img_name in os.listdir(img_dir):\n",
    "            if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                label_name = os.path.splitext(img_name)[0] + '.txt'\n",
    "                if not os.path.exists(os.path.join(label_dir, label_name)):\n",
    "                    os.remove(os.path.join(img_dir, img_name))\n",
    "\n",
    "    def _load_images_and_labels(self, img_dir, label_dir):\n",
    "        for img_name in os.listdir(img_dir):\n",
    "            if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                img_path = os.path.join(img_dir, img_name)\n",
    "                label_path = os.path.join(label_dir, os.path.splitext(img_name)[0] + '.txt')\n",
    "\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "\n",
    "                if os.path.exists(label_path):\n",
    "                    with open(label_path, 'r') as f:\n",
    "                        labels = [list(map(float, line.strip().split())) for line in f if line.strip()]\n",
    "                else:\n",
    "                    labels = []\n",
    "\n",
    "                self.images.append(img)\n",
    "                self.labels.append(torch.tensor(labels, dtype=torch.float32))\n",
    "\n",
    "    def _prepare_dataset(self):\n",
    "        image_dir = os.path.join(self.data_dir, 'images', self.mode)\n",
    "        label_dir = os.path.join(self.data_dir, 'labels', self.mode)\n",
    "\n",
    "        self._check_and_clean(image_dir, label_dir)\n",
    "        self._load_images_and_labels(image_dir, label_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]\n",
    "\n",
    "# Example usage\n",
    "transform = T.Compose([\n",
    "    T.Resize((320, 320)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "data_dir = \"/mnt/d/object_detect_tracking/camera-viewer/data/brain_tumor_copy/axial_t1wce_2_class\"\n",
    "train_dataset = YoloDataset(data_dir, img_size=320, transform=transform, mode='train')\n",
    "test_dataset = YoloDataset(data_dir, img_size=320, transform=transform, mode='test')\n",
    "\n",
    "# DataLoader ready for training:\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f4f9e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 296\n",
      "Number of testing samples: 75\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training samples:\", len(train_dataset))\n",
    "print(\"Number of testing samples:\", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc044117",
   "metadata": {},
   "source": [
    "# SIoU Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27c10266",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import torch\n",
    "import torch.nn as nn\n",
    "# import numpy as np\n",
    " \n",
    " \n",
    "class SIoU(nn.Module):\n",
    "        # SIoU Loss https://arxiv.org/pdf/2205.12740.pdf\n",
    "    def __init__(self, x1y1x2y2=True, eps=1e-7):\n",
    "        super(SIoU, self).__init__()\n",
    "        self.x1y1x2y2 = x1y1x2y2\n",
    "        self.eps = eps\n",
    "    \n",
    "    \n",
    "            \n",
    "    def forward(self, box1, box2):\n",
    "    \n",
    "        # Get the coordinates of bounding boxes\n",
    "        if self.x1y1x2y2:  # x1, y1, x2, y2 = box1\n",
    "            b1_x1, b1_y1, b1_x2, b1_y2 = box1[0], box1[1], box1[2], box1[3]\n",
    "            b2_x1, b2_y1, b2_x2, b2_y2 = box2[0], box2[1], box2[2], box2[3]\n",
    "        else:  # transform from xywh to xyxy\n",
    "            b1_x1, b1_x2 = box1[0] - box1[2] / 2, box1[0] + box1[2] / 2\n",
    "            b1_y1, b1_y2 = box1[1] - box1[3] / 2, box1[1] + box1[3] / 2\n",
    "            b2_x1, b2_x2 = box2[0] - box2[2] / 2, box2[0] + box2[2] / 2\n",
    "            b2_y1, b2_y2 = box2[1] - box2[3] / 2, box2[1] + box2[3] / 2\n",
    "    \n",
    "    \n",
    "        # Intersection area\n",
    "        inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \\\n",
    "                (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)\n",
    "    \n",
    "    \n",
    "        # Union Area\n",
    "        w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + self.eps\n",
    "        w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + self.eps\n",
    "        union = w1 * h1 + w2 * h2 - inter + self.eps\n",
    "    \n",
    "        # IoU value of the bounding boxes\n",
    "        iou = inter / union\n",
    "        cw = torch.max(b1_x2, b2_x2) - torch.min(b1_x1, b2_x1)  # convex (smallest enclosing box) width\n",
    "        ch = torch.max(b1_y2, b2_y2) - torch.min(b1_y1, b2_y1)  # convex height\n",
    "        s_cw = (b2_x1 + b2_x2 - b1_x1 - b1_x2) * 0.5\n",
    "        s_ch = (b2_y1 + b2_y2 - b1_y1 - b1_y2) * 0.5\n",
    "        sigma = torch.pow(s_cw ** 2 + s_ch ** 2, 0.5) + self.eps\n",
    "        sin_alpha_1 = torch.abs(s_cw) / sigma\n",
    "        sin_alpha_2 = torch.abs(s_ch) / sigma\n",
    "        threshold = pow(2, 0.5) / 2\n",
    "        sin_alpha = torch.where(sin_alpha_1 > threshold, sin_alpha_2, sin_alpha_1)\n",
    "            \n",
    "        # Angle Cost\n",
    "        angle_cost = 1 - 2 * torch.pow( torch.sin(torch.arcsin(sin_alpha) - np.pi/4), 2)\n",
    "            \n",
    "        # Distance Cost\n",
    "        rho_x = (s_cw / (cw + self.eps)) ** 2\n",
    "        rho_y = (s_ch / (ch + self.eps)) ** 2\n",
    "        gamma = 2 - angle_cost\n",
    "        distance_cost = 2 - torch.exp(gamma * rho_x) - torch.exp(gamma * rho_y)\n",
    "            \n",
    "        # Shape Cost\n",
    "        omiga_w = torch.abs(w1 - w2) / torch.max(w1, w2)\n",
    "        omiga_h = torch.abs(h1 - h2) / torch.max(h1, h2)\n",
    "\n",
    "\n",
    "        print(\"omiga_w:\", omiga_w)\n",
    "        print(\"omiga_h:\", omiga_h)\n",
    "        print(\"distance_cost:\", distance_cost)\n",
    "        print(\"angle_cost:\", angle_cost)\n",
    "        shape_cost = torch.pow(1 - torch.exp(-1 * omiga_w), 4) + torch.pow(1 - torch.exp(-1 * omiga_h), 4)\n",
    "        print(\"shape_cost:\", shape_cost)\n",
    "\n",
    "        return 1 - (iou + 0.5 * (distance_cost + shape_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3dac857f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omiga_w: tensor(0.2000)\n",
      "omiga_h: tensor(0.2000)\n",
      "distance_cost: tensor(0.)\n",
      "angle_cost: tensor(5.9605e-08)\n",
      "shape_cost: tensor(0.0022)\n",
      "tensor(0.3589)\n",
      "SIoU Loss: 0.3589203357696533\n"
     ]
    }
   ],
   "source": [
    "siou = SIoU(x1y1x2y2=True)\n",
    "\n",
    "box1 = torch.tensor([50, 50, 150, 150], dtype=torch.float32)  # x1, y1, x2, y2\n",
    "box2 = torch.tensor([60, 60, 140, 140], dtype=torch.float32)\n",
    "\n",
    "loss = siou(box1, box2)\n",
    "print(loss)\n",
    "print(\"SIoU Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04b2497",
   "metadata": {},
   "source": [
    "# Show  model infomation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cee9d73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 25786365\n",
      "Model Memory Size (MB): 98.3671760559082\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torchvision import transforms\n",
    "from model.yolo.yolo_net import YOLONet\n",
    "\n",
    "model = YOLONet(num_classes=2, num_anchors=9)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# show size of the model\n",
    "print(\"Model size:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "print(\"Model Memory Size (MB):\", sum(p.numel() for p in model.parameters() if p.requires_grad) * 4 / (1024 * 1024))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1d1f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
