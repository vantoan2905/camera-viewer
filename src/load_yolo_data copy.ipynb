{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c1518a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Check CUDA\n",
    "print(\"Torch CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509bcd28",
   "metadata": {},
   "source": [
    "# ---------------------- Dataset ----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efbf2a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class YoloDataset(Dataset):\n",
    "    def __init__(self, data_dir, img_size=320, transform=None, mode='train'):\n",
    "        self.data_dir = data_dir\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self._prepare_dataset()\n",
    "\n",
    "    def _check_and_clean(self, img_dir, label_dir):\n",
    "        for img_name in os.listdir(img_dir):\n",
    "            if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                label_name = os.path.splitext(img_name)[0] + '.txt'\n",
    "                if not os.path.exists(os.path.join(label_dir, label_name)):\n",
    "                    os.remove(os.path.join(img_dir, img_name))\n",
    "\n",
    "    def _load_images_and_labels(self, img_dir, label_dir):\n",
    "        for img_name in os.listdir(img_dir):\n",
    "            if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                img_path = os.path.join(img_dir, img_name)\n",
    "                label_path = os.path.join(label_dir, os.path.splitext(img_name)[0] + '.txt')\n",
    "\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "\n",
    "                if os.path.exists(label_path):\n",
    "                    with open(label_path, 'r') as f:\n",
    "                        labels = [list(map(float, line.strip().split())) for line in f if line.strip()]\n",
    "                else:\n",
    "                    labels = []\n",
    "\n",
    "                self.images.append(img)\n",
    "                self.labels.append(torch.tensor(labels, dtype=torch.float32))\n",
    "\n",
    "    def _prepare_dataset(self):\n",
    "        image_dir = os.path.join(self.data_dir, 'images', self.mode)\n",
    "        label_dir = os.path.join(self.data_dir, 'labels', self.mode)\n",
    "        self._check_and_clean(image_dir, label_dir)\n",
    "        self._load_images_and_labels(image_dir, label_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f8c726f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 296\n",
      "Number of testing samples: 75\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Transform and DataLoader setup\n",
    "transform = T.Compose([\n",
    "    T.Resize((320, 320)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "data_dir = \"/mnt/d/object_detect_tracking/data/brain_tumor_copy/axial_t1wce_2_class\"\n",
    "train_dataset = YoloDataset(data_dir, img_size=320, transform=transform, mode='train')\n",
    "test_dataset = YoloDataset(data_dir, img_size=320, transform=transform, mode='test')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)\n",
    "\n",
    "print(\"Number of training samples:\", len(train_dataset))\n",
    "print(\"Number of testing samples:\", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fafcd2",
   "metadata": {},
   "source": [
    "# ---------------------- SIoU Loss ----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb4be2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SIoU(nn.Module):\n",
    "    def __init__(self, x1y1x2y2=True, eps=1e-7):\n",
    "        super(SIoU, self).__init__()\n",
    "        self.x1y1x2y2 = x1y1x2y2\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, box1, box2):\n",
    "        if self.x1y1x2y2:\n",
    "            b1_x1, b1_y1, b1_x2, b1_y2 = box1\n",
    "            b2_x1, b2_y1, b2_x2, b2_y2 = box2\n",
    "        else:\n",
    "            b1_x1, b1_x2 = box1[0] - box1[2] / 2, box1[0] + box1[2] / 2\n",
    "            b1_y1, b1_y2 = box1[1] - box1[3] / 2, box1[1] + box1[3] / 2\n",
    "            b2_x1, b2_x2 = box2[0] - box2[2] / 2, box2[0] + box2[2] / 2\n",
    "            b2_y1, b2_y2 = box2[1] - box2[3] / 2, box2[1] + box2[3] / 2\n",
    "\n",
    "        inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \\\n",
    "                (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)\n",
    "\n",
    "        w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + self.eps\n",
    "        w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + self.eps\n",
    "        union = w1 * h1 + w2 * h2 - inter + self.eps\n",
    "\n",
    "        iou = inter / union\n",
    "        cw = torch.max(b1_x2, b2_x2) - torch.min(b1_x1, b2_x1)\n",
    "        ch = torch.max(b1_y2, b2_y2) - torch.min(b1_y1, b2_y1)\n",
    "        s_cw = (b2_x1 + b2_x2 - b1_x1 - b1_x2) * 0.5\n",
    "        s_ch = (b2_y1 + b2_y2 - b1_y1 - b1_y2) * 0.5\n",
    "        sigma = torch.sqrt(s_cw ** 2 + s_ch ** 2) + self.eps\n",
    "\n",
    "        sin_alpha_1 = torch.abs(s_cw) / sigma\n",
    "        sin_alpha_2 = torch.abs(s_ch) / sigma\n",
    "        threshold = np.sqrt(2) / 2\n",
    "        sin_alpha = torch.where(sin_alpha_1 > threshold, sin_alpha_2, sin_alpha_1)\n",
    "\n",
    "        angle_cost = 1 - 2 * torch.pow(torch.sin(torch.arcsin(sin_alpha) - np.pi / 4), 2)\n",
    "        rho_x = (s_cw / (cw + self.eps)) ** 2\n",
    "        rho_y = (s_ch / (ch + self.eps)) ** 2\n",
    "        gamma = 2 - angle_cost\n",
    "        distance_cost = 2 - torch.exp(gamma * rho_x) - torch.exp(gamma * rho_y)\n",
    "\n",
    "        omiga_w = torch.abs(w1 - w2) / torch.max(w1, w2)\n",
    "        omiga_h = torch.abs(h1 - h2) / torch.max(h1, h2)\n",
    "        shape_cost = torch.pow(1 - torch.exp(-omiga_w), 4) + torch.pow(1 - torch.exp(-omiga_h), 4)\n",
    "\n",
    "        return 1 - (iou + 0.5 * (distance_cost + shape_cost))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8efaeb",
   "metadata": {},
   "source": [
    "# Example SIoU loss usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "df1548c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIoU Loss: 0.3589203357696533\n"
     ]
    }
   ],
   "source": [
    "\n",
    "siou = SIoU(x1y1x2y2=True)\n",
    "box1 = torch.tensor([50, 50, 150, 150], dtype=torch.float32)\n",
    "box2 = torch.tensor([60, 60, 140, 140], dtype=torch.float32)\n",
    "loss = siou(box1, box2)\n",
    "print(\"SIoU Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42744f1",
   "metadata": {},
   "source": [
    "# ---------------------- Training model ----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2eb396fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Giả sử bạn đã có:\n",
    "# - model: YOLO model đã khởi tạo\n",
    "# - train_loader, test_loader: DataLoader đã sẵn sàng\n",
    "# - criterion: YOLO Loss hoặc BCE/CIoU loss phù hợp\n",
    "# - optimizer: Adam hoặc SGD\n",
    "\n",
    "\n",
    "class SIoU(nn.Module):\n",
    "    def __init__(self, x1y1x2y2=True, eps=1e-7):\n",
    "        super(SIoU, self).__init__()\n",
    "        self.x1y1x2y2 = x1y1x2y2\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, box1, box2):\n",
    "        if self.x1y1x2y2:\n",
    "            b1_x1, b1_y1, b1_x2, b1_y2 = box1\n",
    "            b2_x1, b2_y1, b2_x2, b2_y2 = box2\n",
    "        else:\n",
    "            b1_x1, b1_x2 = box1[0] - box1[2] / 2, box1[0] + box1[2] / 2\n",
    "            b1_y1, b1_y2 = box1[1] - box1[3] / 2, box1[1] + box1[3] / 2\n",
    "            b2_x1, b2_x2 = box2[0] - box2[2] / 2, box2[0] + box2[2] / 2\n",
    "            b2_y1, b2_y2 = box2[1] - box2[3] / 2, box2[1] + box2[3] / 2\n",
    "\n",
    "        inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \\\n",
    "                (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)\n",
    "\n",
    "        w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + self.eps\n",
    "        w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + self.eps\n",
    "        union = w1 * h1 + w2 * h2 - inter + self.eps\n",
    "\n",
    "        iou = inter / union\n",
    "        cw = torch.max(b1_x2, b2_x2) - torch.min(b1_x1, b2_x1)\n",
    "        ch = torch.max(b1_y2, b2_y2) - torch.min(b1_y1, b2_y1)\n",
    "        s_cw = (b2_x1 + b2_x2 - b1_x1 - b1_x2) * 0.5\n",
    "        s_ch = (b2_y1 + b2_y2 - b1_y1 - b1_y2) * 0.5\n",
    "        sigma = torch.sqrt(s_cw ** 2 + s_ch ** 2) + self.eps\n",
    "\n",
    "        sin_alpha_1 = torch.abs(s_cw) / sigma\n",
    "        sin_alpha_2 = torch.abs(s_ch) / sigma\n",
    "        threshold = np.sqrt(2) / 2\n",
    "        sin_alpha = torch.where(sin_alpha_1 > threshold, sin_alpha_2, sin_alpha_1)\n",
    "\n",
    "        angle_cost = 1 - 2 * torch.pow(torch.sin(torch.arcsin(sin_alpha) - np.pi / 4), 2)\n",
    "        rho_x = (s_cw / (cw + self.eps)) ** 2\n",
    "        rho_y = (s_ch / (ch + self.eps)) ** 2\n",
    "        gamma = 2 - angle_cost\n",
    "        distance_cost = 2 - torch.exp(gamma * rho_x) - torch.exp(gamma * rho_y)\n",
    "\n",
    "        omiga_w = torch.abs(w1 - w2) / torch.max(w1, w2)\n",
    "        omiga_h = torch.abs(h1 - h2) / torch.max(h1, h2)\n",
    "        shape_cost = torch.pow(1 - torch.exp(-omiga_w), 4) + torch.pow(1 - torch.exp(-omiga_h), 4)\n",
    "\n",
    "        return 1 - (iou + 0.5 * (distance_cost + shape_cost))\n",
    "\n",
    "\n",
    "\n",
    "# using if pred_boxes and true_boxes are many boxes in one image\n",
    "def caculate_loss_many_boxes(pred_boxes, true_boxes):\n",
    "    losses = []\n",
    "    for i in range(len(pred_boxes)):\n",
    "        pred_box = pred_boxes[i]\n",
    "        true_box = true_boxes[i]\n",
    "        loss = siou(pred_box, true_box)\n",
    "        losses.append(loss)\n",
    "    return losses\n",
    "\n",
    "# using if pred_boxes and true_boxes are one box in one image\n",
    "def yolo_decode_output(output, num_classes=2, anchors=None, stride=None):\n",
    "    \"\"\"\n",
    "    Decode YOLO output thành list bbox dự đoán [B, N, 6] (cx, cy, w, h, obj_conf, class_pred).\n",
    "    \n",
    "    Args:\n",
    "        output: Tensor đầu ra YOLO [B, A*(5+C), H, W]\n",
    "        num_classes: số class\n",
    "        anchors: tensor anchors shape [A, 2] (w, h), normalized hoặc theo stride\n",
    "        stride: stride tại scale hiện tại, nếu cần scale anchors\n",
    "        \n",
    "    Returns:\n",
    "        pred_bboxes: Tensor [B, N, 6] (cx, cy, w, h, obj_conf, class_id)\n",
    "    \"\"\"\n",
    "    B, C, H, W = output.shape\n",
    "    num_anchors = C // (5 + num_classes)\n",
    "\n",
    "    output = output.view(B, num_anchors, 5 + num_classes, H, W)\n",
    "    output = output.permute(0, 1, 3, 4, 2).contiguous()  # [B, A, H, W, 5+C]\n",
    "\n",
    "    # Sigmoid cho tx, ty, obj_conf, class\n",
    "    tx = torch.sigmoid(output[..., 0])\n",
    "    ty = torch.sigmoid(output[..., 1])\n",
    "    tw = output[..., 2]\n",
    "    th = output[..., 3]\n",
    "    obj_conf = torch.sigmoid(output[..., 4])\n",
    "    class_pred = torch.softmax(output[..., 5:], dim=-1)\n",
    "\n",
    "    # Tạo grid\n",
    "    grid_y, grid_x = torch.meshgrid(torch.arange(H), torch.arange(W), indexing=\"ij\")\n",
    "    grid_x = grid_x.to(output.device).float()\n",
    "    grid_y = grid_y.to(output.device).float()\n",
    "\n",
    "    # Nếu cần anchors\n",
    "    if anchors is not None:\n",
    "        anchors = anchors.to(output.device)\n",
    "        if stride is not None:\n",
    "            anchors = anchors / stride\n",
    "        anchors = anchors.view(1, num_anchors, 1, 1, 2)  # [1, A, 1, 1, 2]\n",
    "\n",
    "        # scale bbox\n",
    "        bx = (tx + grid_x.unsqueeze(0).unsqueeze(0)) / W\n",
    "        by = (ty + grid_y.unsqueeze(0).unsqueeze(0)) / H\n",
    "        bw = (torch.exp(tw) * anchors[..., 0]) / W\n",
    "        bh = (torch.exp(th) * anchors[..., 1]) / H\n",
    "    else:\n",
    "        # nếu anchors không được cung cấp, trả bbox relative grid\n",
    "        bx = (tx + grid_x.unsqueeze(0).unsqueeze(0)) / W\n",
    "        by = (ty + grid_y.unsqueeze(0).unsqueeze(0)) / H\n",
    "        bw = torch.exp(tw) / W\n",
    "        bh = torch.exp(th) / H\n",
    "\n",
    "    # Lấy class id\n",
    "    class_id = torch.argmax(class_pred, dim=-1).float()  # [B, A, H, W]\n",
    "\n",
    "    # Stack lại [B, A, H, W, 6]\n",
    "    pred = torch.stack([bx, by, bw, bh, obj_conf, class_id], dim=-1)\n",
    "\n",
    "    # reshape về [B, N, 6]\n",
    "    pred = pred.view(B, -1, 6)\n",
    "\n",
    "    return pred\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a617db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_calculation(pred_boxes, target_boxes):\n",
    "    \n",
    "    losses = []\n",
    "    for bbox in pred_boxes:\n",
    "        loss = siou(bbox, target_boxes)\n",
    "        losses.append(loss)\n",
    "    losses = losses.mean()\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e78e900d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (parameters): 25786365\n",
      "Model memory size (MB): 98.3671760559082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30:   0%|          | 0/37 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_boxes: torch.Size([3600, 4])\n",
      "t_boxes: torch.Size([1, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30:   0%|          | 0/37 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp_boxes:\u001b[39m\u001b[38;5;124m\"\u001b[39m, p_boxes\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt_boxes:\u001b[39m\u001b[38;5;124m\"\u001b[39m, t_boxes\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 56\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43msiou\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_boxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_boxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[38], line 9\u001b[0m, in \u001b[0;36mSIoU.forward\u001b[0;34m(self, box1, box2)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, box1, box2):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx1y1x2y2:\n\u001b[0;32m----> 9\u001b[0m         b1_x1, b1_y1, b1_x2, b1_y2 \u001b[38;5;241m=\u001b[39m box1\n\u001b[1;32m     10\u001b[0m         b2_x1, b2_y1, b2_x2, b2_y2 \u001b[38;5;241m=\u001b[39m box2\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "\n",
    "from model.yolo.yolo_net import YOLONet\n",
    "import torchvision.ops as ops\n",
    "\n",
    "model = YOLONet(num_classes=2, num_anchors=9)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model_size = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "model_mem_mb = model_size * 4 / (1024 * 1024)\n",
    "print(\"Model size (parameters):\", model_size)\n",
    "print(\"Model memory size (MB):\", model_mem_mb)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for imgs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        imgs = imgs.to(device)\n",
    "        targets = [t.to(device) for t in targets]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "\n",
    "\n",
    "\n",
    "        out_13, out_26, out_52 = model(imgs)\n",
    "\n",
    "        pred13 = yolo_decode_output(out_13, num_classes=2)\n",
    "        pred26 = yolo_decode_output(out_26, num_classes=2)\n",
    "        pred52 = yolo_decode_output(out_52, num_classes=2)\n",
    "        for bbox in targets:\n",
    "            \n",
    "            for i in range(pred13.shape[0]):\n",
    "                p = pred13[i]  # [N, 6]\n",
    "\n",
    "                if bbox.shape[0] == 0:\n",
    "                    continue\n",
    "\n",
    "                # convert cxcywh -> xyxy\n",
    "                p_boxes = ops.box_convert(p[:, :4], in_fmt=\"cxcywh\", out_fmt=\"xyxy\")\n",
    "                t_boxes = ops.box_convert(bbox[:, :4], in_fmt=\"cxcywh\", out_fmt=\"xyxy\")\n",
    "                print(\"p_boxes:\", p_boxes.shape)\n",
    "                print(\"t_boxes:\", t_boxes.shape)\n",
    "                \n",
    "                loss = siou(p_boxes, t_boxes)\n",
    "                print(\"loss:\", loss.item())\n",
    "                break\n",
    "            break\n",
    "        print(\"Decoded output shapes:\", pred13.shape)\n",
    "        # print(\"Targets shapes:\", targets.shape)\n",
    "        # loss = caculate_loss_many_boxes(output_13,targets)\n",
    "        break\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    break  # Uncomment this line to test only one epoch\n",
    "\n",
    "\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}\")\n",
    "    # break  for test one epoch\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, targets in test_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            targets = [t.to(device) for t in targets]\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            loss = torch.rand(1).to(device)  # thay bằng loss YOLO thực tế\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Lưu checkpoint\n",
    "    torch.save(model.state_dict(), f\"yolo_epoch_{epoch+1}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97be328",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
